# Knowledge Distillation for Image Classification

## ğŸ“Œ Overview
This project demonstrates **Knowledge Distillation (KD)** for compressing a large CNN (Teacher) into a smaller CNN (Student) while maintaining competitive accuracy.  
We experimented on **CIFAR-10** and **Fashion-MNIST** datasets.

## ğŸ§  Approach
- Train a **Teacher Model** (deep CNN) on the dataset.
- Distill knowledge into a **Student Model**:
  - Soft labels from the Teacher.
  - Hard labels from the dataset.
  - Optional feature distillation.
- Evaluate performance on metrics such as accuracy, inference time, entropy, KL divergence, and compression ratio.

## ğŸ“Š Results (Sample)
| Metric                | Teacher Model | Student Model |
|------------------------|---------------|---------------|
| Parameters             | 14.9M         | 357K          |
| Compression Ratio      | â€“             | 41.88Ã— smaller|
| Accuracy (example)     | ~92%          | ~89%          |
| Inference Time (ms)    | 87.83         | 89.90         |
| KL Divergence          | â€“             | 0.4666        |

## ğŸ“‚ Datasets
- CIFAR-10  
- Fashion-MNIST  

## ğŸš€ Requirements
- Python 3.x
- TensorFlow / PyTorch
- Numpy, Matplotlib, etc.  

Install dependencies:
pip install -r requirements.txt

## â–¶ï¸ Usage
1. Train the Teacher model:
   python train_teacher.py

2. Distill into Student model:
   python distill_student.py

3. Evaluate:
   python evaluate.py

## ğŸ“Œ Future Work
- Add pruning/quantization after distillation.
- Test on more complex datasets.
- Deploy Student model on edge devices.

## ğŸ™Œ Credits
Team Members: (Add names here)  
Inspired by Hinton et al., Distilling the Knowledge in a Neural Network (2015)
